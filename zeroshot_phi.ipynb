{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "from datasets import Dataset\n",
    "from peft import LoraConfig, PeftConfig\n",
    "from trl import SFTTrainer\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, BitsAndBytesConfig, pipeline, logging\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import load_dataset, Dataset\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "# os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=\"bentrevett/multi30k\"\n",
    "model_id=\"microsoft/phi-2\"\n",
    "output_model=\"phi2-multi30k-v1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/microsoft/phi-2:\n",
      "- configuration_phi.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "A new version of the following files was downloaded from https://huggingface.co/microsoft/phi-2:\n",
      "- modeling_phi.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd2849bb41e44384a92bdd0610075e51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29a9a8919fc44977bd4b04a2c5fc943e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruct: translate English to German.\n",
      "Input: the guy with the orange hat is staring at something.\n",
      "Output: Der Mann mit der orange Haut ist ganz zurückgehend.\n",
      "<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "torch.set_default_device(\"cuda\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"microsoft/phi-2\", torch_dtype=\"auto\", trust_remote_code=True, device_map='auto')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-2\", trust_remote_code=True)\n",
    "# Instruct chat template chat template\n",
    "#inputs = tokenizer(\"Instruct: translate the guy with the orange hat is staring at something to german\\nOutput:\", return_tensors=\"pt\", return_attention_mask=False)\n",
    "\n",
    "inputs = tokenizer(\"Instruct: translate English to German.\\nInput: the guy with the orange hat is staring at something.\\nOutput:\", return_tensors=\"pt\", return_attention_mask=False)\n",
    "outputs = model.generate(**inputs, max_length=200)\n",
    "text = tokenizer.batch_decode(outputs)[0]\n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GenerationConfig\n",
    "from time import perf_counter\n",
    "\n",
    "def formatted_prompt(question)-> str:\n",
    "    return f\"Instruct: Translate German to English.\\nInput: {question}\\nOutput:\"\n",
    "\n",
    "def generate_response(user_input):\n",
    "\n",
    "  prompt = formatted_prompt(user_input)\n",
    "\n",
    "  inputs = tokenizer([prompt], return_tensors=\"pt\")\n",
    "  generation_config = GenerationConfig(penalty_alpha=0.6,do_sample = True,\n",
    "      top_k=5,temperature=0.5,repetition_penalty=1.2,\n",
    "      max_new_tokens=20,pad_token_id=tokenizer.eos_token_id\n",
    "  )\n",
    "  start_time = perf_counter()\n",
    "\n",
    "  inputs = tokenizer(prompt, return_tensors=\"pt\").to('cuda')\n",
    "\n",
    "  outputs = model.generate(**inputs, generation_config=generation_config)\n",
    "  output=tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "  #print(output)\n",
    "  output_time = perf_counter() - start_time\n",
    "  #print(f\"Time taken for inference: {round(output_time,2)} seconds\")\n",
    "  return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = load_dataset(dataset, split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [09:26<00:00,  1.77it/s]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "mydf = pd.DataFrame(columns=['de','en','predicted_de'])\n",
    "for i in tqdm(range(0,len(data_test))):\n",
    "    out=generate_response(user_input=data_test[i]['de'])\n",
    "    out=out.split('Output: ')[1]\n",
    "    #processed_out=re.sub(\"[^A-Za-z0-9., ]\",\"\",out)\n",
    "    mydf=pd.concat([mydf,pd.DataFrame({'de':data_test[i]['de'],'en':data_test[i]['en'],'predicted_en':out},index=[0])],ignore_index=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydf.to_csv('phi2-zeroshot-multi30k-v1-GER-EN.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-intro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
